1
00:00:05,680 --> 00:00:06,720
The pipeline function.   The pipeline function is the most  high-level API of the Transformers library.   It regroups together all the steps to go from raw  texts to usable predictions. The model used is at   the core of a pipeline, but the pipeline also  include all the necessary pre-processing (since   the model does not expect texts, but numbers) as  well as some post-processing to make the output of   the model human-readable. Let's look at a first  example with the sentiment analysis pipeline.   This pipeline performs text classification on a  given input, and determines if it's positive or   negative. Here, it attributed the positive label  on the given text, with a confidence of 95%.   You can pass multiple texts to the  same pipeline, which will be processed   and passed through the model together, as a  batch. The output is a list of individual results,   in the same order as the input texts. Here we  find the same label and score for the first text,   and the second text is judged  positive with a confidence of 99.99%.   The zero-shot classification pipeline is a  more general text-classification pipeline:   it allows you to provide the labels you  want. Here we want to classify our input   text along the labels "education", "politics" and  "business". The pipeline successfully recognizes   it's more about education than the  other labels, with a confidence of 84%.   Moving on to other tasks, the text generation  pipeline will auto-complete a given prompt. The   output is generated with a bit of randomness, so  it changes each time you call the generator object   on a given prompt. Up until now, we have used the  pipeline API with the default model associated to   each task, but you can use it with any model that  has been pretrained or fine-tuned on this task.   Going on the model hub (huggingface.co/models),  you can filter the available models by task.   The default model used in our  previous example was gpt2,   but there are many more models  available, and not just in English!   Let's go back to the text generation pipeline and  load it with another model, distilgpt2. This is   a lighter version of gpt2 created by the Hugging  Face team. When applying the pipeline to a given   prompt, we can specify several arguments, such as  the maximum length of the generated texts, or the   number of sentences we want to return (since  there is some randomness in the generation).   Generating text by guessing the next word in a  sentence was the pretraining objective of GPT-2,   the fill mask pipeline is the pretraining  objective of BERT, which is to guess the value   of masked word. In this case, we ask the two most  likely values for the missing words (according to   the model) and get mathematical or computational  as possible answers. Another task Transformers   model can perform is to classify each word in  the sentence instead of the sentence as a whole.   One example of this is Named Entity Recognition,  which is the task of identifying entities, such as   persons, organizations or locations in a sentence.  Here, the model correctly finds the person   (Sylvain), the organization (Hugging Face) as well  as the location (Brooklyn) inside the input text.   The grouped_entities=True argument used  is to make the pipeline group together   the different words linked to the same  entity (such as Hugging and Face here).   Another task available with the pipeline  API is extractive question answering.   Providing a context and a question, the model  will identify the span of text in the context   containing the answer to the question. Getting  short summaries of very long articles is   also something the Transformers library can  help with, with the summarization pipeline.   Finally, the last task supported by the  pipeline API is translation. Here we use   a French/English model found on the model hub  to get the English version of our input text.   Here is a brief summary of all the  tasks we looked into in this video.   Try then out through the inference  widgets in the model hub!

2
00:00:09,360 --> 00:00:13,280



3
00:00:13,840 --> 00:00:21,200



4
00:00:21,200 --> 00:00:26,720



5
00:00:26,720 --> 00:00:32,800



6
00:00:32,800 --> 00:00:39,440



7
00:00:40,480 --> 00:00:46,080



8
00:00:46,080 --> 00:00:53,120



9
00:00:55,440 --> 00:00:59,520



10
00:00:59,520 --> 00:01:05,840



11
00:01:05,840 --> 00:01:12,080



12
00:01:12,080 --> 00:01:16,480



13
00:01:18,480 --> 00:01:22,720



14
00:01:23,360 --> 00:01:28,320



15
00:01:28,320 --> 00:01:35,360



16
00:01:35,360 --> 00:01:39,360



17
00:01:41,440 --> 00:01:47,360



18
00:01:47,360 --> 00:01:52,560



19
00:01:52,560 --> 00:01:58,960



20
00:01:58,960 --> 00:02:03,920



21
00:02:06,320 --> 00:02:12,320



22
00:02:13,120 --> 00:02:16,960



23
00:02:16,960 --> 00:02:20,080



24
00:02:21,280 --> 00:02:27,120



25
00:02:27,120 --> 00:02:33,120



26
00:02:33,120 --> 00:02:39,280



27
00:02:39,280 --> 00:02:43,520



28
00:02:45,920 --> 00:02:50,480



29
00:02:51,200 --> 00:02:56,240



30
00:02:56,240 --> 00:03:02,480



31
00:03:02,480 --> 00:03:09,120



32
00:03:09,120 --> 00:03:13,920



33
00:03:14,720 --> 00:03:21,040



34
00:03:21,040 --> 00:03:29,360



35
00:03:29,360 --> 00:03:36,000



36
00:03:37,440 --> 00:03:42,080



37
00:03:42,080 --> 00:03:46,080



38
00:03:48,000 --> 00:03:52,160



39
00:03:52,720 --> 00:03:58,080



40
00:03:58,080 --> 00:04:03,920



41
00:04:03,920 --> 00:04:07,840



42
00:04:09,360 --> 00:04:15,040



43
00:04:15,040 --> 00:04:19,440



44
00:04:21,360 --> 00:04:24,720



45
00:04:25,280 --> 00:04:27,840


